{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "from scipy import stats\n",
    "import corner\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from mlxtend.data import boston_housing_data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Since I have already preprocessed the data and used `sklearn train_test_split` we have three separate files needed to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X.csv')\n",
    "Y_train = pd.read_csv('y.csv')\n",
    "\n",
    "X_test = pd.read_csv('test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Multicollinearity is very important without doing so its likely that we would be overfitting. However, I feel like this is only towards the case with much simpler models such as linear regression and the fact that we run some sort of boosted trees won't really affect much. But it is good to identify some of these predictor variables that have high VIFs (Variance Inflation Factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIF Factor</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5</td>\n",
       "      <td>LotFrontage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.4</td>\n",
       "      <td>LotArea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.6</td>\n",
       "      <td>Alley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.6</td>\n",
       "      <td>LotShape</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VIF Factor     features\n",
       "0         3.5  LotFrontage\n",
       "1         4.4      LotArea\n",
       "2         1.5       Street\n",
       "3         6.6        Alley\n",
       "4         1.6     LotShape"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "vif[\"features\"] = X_train.columns\n",
    "vif.round(1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Its likely that we have to remove some variables that have VIF values over 5 so alley is one variable we may need to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Mean Squared Error (RMSE) function to evaluate results\n",
    "* RMSE represents the sample standard deviation of the differences between predicted values and observed values (called residuals).\n",
    "* Since the errors are squared before they are averaged, RMSE gives a relatively high weight to large errors (Penalizes it harder than MAE)\n",
    "\n",
    "* This means the RMSE should be more useful when large errors are particularly undesirable.\n",
    "\n",
    "* From an interpretation standpoint, MAE has the edge. RMSE does not describe average error alone and has other implications that are more difficult to tease out and understand.\n",
    "\n",
    "* However, an advantage of RMSE over MAE is that RMSE avoids the use of taking the absolute value, which is undesirable in terms of calculations and applications\n",
    "\n",
    "* In addition, What are good rmse values?\n",
    "    * This really depends on the depedent Y variables \n",
    "    * when train rmsse is similar to test rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets define some rmse functions\n",
    "* Note most sklearn models have some sort of evaluation metric similar to rmse such as negative mean square error. All we have to do is tweek it slightly and we can get the rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets): \n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "def rmse_cv(model, x_train, y_train):\n",
    "        rmse = np.sqrt(-cross_val_score(model, x_train, y_train, scoring='neg_mean_squared_error', cv=5))\n",
    "        return rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation scores:\n",
      "\n",
      "R^2 Score: 0.90126 (+/- 0.01012) [Linear Regression]\n",
      "RMSE Score: 0.12531 (+/- 0.01015) [Linear Regression] \n",
      "\n",
      "R^2 Score: -0.00335 (+/- 0.00490) [Lasso]\n",
      "RMSE Score: 0.39947 (+/- 0.01584) [Lasso] \n",
      "\n",
      "R^2 Score: 0.91125 (+/- 0.00834) [Ridge]\n",
      "RMSE Score: 0.11876 (+/- 0.00844) [Ridge] \n",
      "\n",
      "R^2 Score: 0.89906 (+/- 0.00922) [SVR]\n",
      "RMSE Score: 0.12669 (+/- 0.00929) [SVR] \n",
      "\n",
      "R^2 Score: 0.84365 (+/- 0.01345) [Random Forest]\n",
      "RMSE Score: 0.15730 (+/- 0.00285) [Random Forest] \n",
      "\n",
      "R^2 Score: 0.92057 (+/- 0.00651) [Elastic Net]\n",
      "RMSE Score: 0.11232 (+/- 0.00673) [Elastic Net] \n",
      "\n",
      "R^2 Score: 0.91336 (+/- 0.00523) [Gradient Boosting]\n",
      "RMSE Score: 0.11744 (+/- 0.00775) [Gradient Boosting] \n",
      "\n",
      "R^2 Score: 0.91513 (+/- 0.00778) [XGBoost Regressor]\n",
      "RMSE Score: 0.11609 (+/- 0.00737) [XGBoost Regressor] \n",
      "\n",
      "R^2 Score: 0.91701 (+/- 0.00453) [Kernel Ridge]\n",
      "RMSE Score: 0.11485 (+/- 0.00561) [Kernel Ridge] \n",
      "\n",
      "R^2 Score: 0.91902 (+/- 0.00692) [StackingCVRegressor]\n",
      "RMSE Score: 0.11343 (+/- 0.00718) [StackingCVRegressor] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 33\n",
    "\n",
    "lasso = Lasso()\n",
    "linear_reg = LinearRegression()\n",
    "ridge = Ridge(random_state=1)\n",
    "svr_linear = SVR(kernel='linear')\n",
    "rf = RandomForestRegressor(n_estimators=5, random_state=seed)\n",
    "ridge = Ridge(random_state=seed)\n",
    "ElasticNet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0003, l1_ratio=.9, random_state=3,max_iter = 10000))\n",
    "GradBoost = GradientBoostingRegressor(n_estimators=3300, learning_rate=0.03,\n",
    "                                   max_depth=6, max_features='sqrt',\n",
    "                                   min_samples_leaf=13, min_samples_split=9, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "xgb = xgb.XGBRegressor(objective= 'reg:squarederror', colsample_bytree=0.4, gamma=0.05, \n",
    "                             learning_rate=0.05, max_depth=5, \n",
    "                             min_child_weight=2, n_estimators=3000,\n",
    "                             reg_alpha=0.5, reg_lambda=0.9,\n",
    "                             subsample=0.5, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "\n",
    "Kernel_Ridge = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "\n",
    "stack = StackingCVRegressor(regressors=(linear_reg, lasso, ridge, svr_linear, rf, ElasticNet, GradBoost, xgb, Kernel_Ridge),\n",
    "                            meta_regressor=xgb,\n",
    "                            random_state=seed)\n",
    "\n",
    "print('5-fold cross validation scores:\\n')\n",
    "for clf, label in zip([linear_reg, lasso, ridge, svr_linear, rf, ElasticNet, GradBoost, xgb, Kernel_Ridge, stack], ['Linear Regression','Lasso', 'Ridge','SVR', 'Random Forest', 'Elastic Net','Gradient Boosting','XGBoost Regressor','Kernel Ridge',  'StackingCVRegressor']):\n",
    "    scores_R2 = cross_val_score(clf, X_train.as_matrix(), Y_train, cv=5)\n",
    "    scores_rmse = np.sqrt(-cross_val_score(clf, X_train.as_matrix(), Y_train, cv=5, scoring='neg_mean_squared_error'))\n",
    "    print(\"R^2 Score: %0.5f (+/- %0.5f) [%s]\" % (scores_R2.mean(), scores_R2.std(), label))\n",
    "    print(\"RMSE Score: %0.5f (+/- %0.5f) [%s] \\n\" % (scores_rmse.mean(), scores_rmse.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a perfect world where computation power is not an issue then we can tune every parameter in all the base learners mentioned above. However, that is not the case and I'll just tune a few to show that it works and may we can get a slighlty better prediction?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = stack.fit(X_train.as_matrix(), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = stack.predict(X_test.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in predictions:\n",
    "    l.append(np.exp(i)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = l\n",
    "predictions  =pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('newest_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>117747.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165011.546875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184652.421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>193491.421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>182182.046875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0  117747.328125\n",
       "1  165011.546875\n",
       "2  184652.421875\n",
       "3  193491.421875\n",
       "4  182182.046875"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
