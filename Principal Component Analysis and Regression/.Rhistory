#observing the principal components
round(pcacar$rotation,2)
biplot(pcacar)
library(MASS)
allcat <- df
allcat$Type  <- as.numeric(allcat$Type)-1
allcat$Type
allcat <- df
allcat <- df
allcat$Type  <- as.numeric(allcat$Type)-1
allcat$Type
library(MASS)
allcat <- df
allcat$Type  <- as.numeric(allcat$Type)-1
allcat$Type
library(MASS)
allcat <- df
allcat$Type  <- as.numeric(allcat$Type)-1
allcat$Type
library(MASS)
allcat <- df
allcat$Type  <- as.numeric(allcat$Type)-1
allcat$Type
library(MASS)
allcat <- df
allcat$Type  <- as.numeric(allcat$Type)-1
allcat$Type
library(MASS)
allcat <- df
allcat$Type  <- as.numeric(allcat$Type)-1
allcat$Type
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv("car93.csv")
numeric <- df[sapply(df,is.numeric)]
pcacar <- prcomp(numeric, scale. = TRUE) #scaled
summary(pcacar)
#observing the principal components
round(pcacar$rotation,2)
biplot(pcacar)
allcat <- df
#allcat$Type  <- as.numeric(allcat$Type)-1
allcat$Type <- as.character(allcat$Type)
allcat$Type[allcat$Type != “Compact”] <- 0
allcat <- df
#allcat$Type  <- as.numeric(allcat$Type)-1
allcat$Type <- as.character(allcat$Type)
allcat$Type[allcat$Type != “Compact”] <- 0
allcat <- df
#allcat$Type  <- as.numeric(allcat$Type)-1
allcat$Type <- as.character(allcat$Type)
allcat$Type[allcat$Type != "Compact""] <- 0
allcat$Type[allcat$Type == "Midsize"] <- 1
library(MASS)
allcat <- df
#allcat$Type  <- as.numeric(allcat$Type)-1
allcat$Type <- as.character(allcat$Type)
allcat$Type[allcat$Type != "Compact"] <- 0
allcat$Type[allcat$Type == "Midsize"] <- 1
allcat$Type[allcat$Type == "Large"] <- 2
allcat$Type[allcat$Type == "Small"] <- 3
allcat$Type[allcat$Type == "Sporty"] <- 4
allcat$Type <- as.numeric(allcat$Type)
allcat$Type
allcat <- df
#allcat$Type  <- as.numeric(allcat$Type)-1
allcat$Type <- as.character(allcat$Type)
allcat$Type[allcat$Type != "Compact"] <- 0
allcat$Type[allcat$Type == "Midsize"] <- 1
allcat$Type[allcat$Type == "Large"] <- 2
allcat$Type[allcat$Type == "Small"] <- 3
allcat$Type[allcat$Type == "Sporty"] <- 4
allcat$Type
library(MASS)
allcat <- df
#allcat$Type  <- as.numeric(allcat$Type)-1
allcat$Type <- as.character(allcat$Type)
allcat$Type[allcat$Type == "Compact"] <- 0
allcat$Type[allcat$Type == "Midsize"] <- 1
allcat$Type[allcat$Type == "Large"] <- 2
allcat$Type[allcat$Type == "Small"] <- 3
allcat$Type[allcat$Type == "Sporty"] <- 4
allcat$Type
allcat$Type <- as.numeric(allcat$Type)
allcat$Type
library(MASS)
allcat <- df
allcat$Type <- as.character(allcat$Type)
allcat$Type[allcat$Type == "Compact"] <- 0
allcat$Type[allcat$Type == "Midsize"] <- 1
allcat$Type[allcat$Type == "Large"] <- 2
allcat$Type[allcat$Type == "Small"] <- 3
allcat$Type[allcat$Type == "Sporty"] <- 4
allcat$Type <- as.numeric(allcat$Type)
x <- pcacar$x[,1:2]
y <- allcat$Type
alldat <- data.frame(x,y)
alllda <- lda(y~., data=alldat, CV=TRUE)
MultiLogLoss(y_true = y, y_pred = alllda$posterior)
table(allcat$Type, alllda$class)
library(MASS)
carlda<- lda(y~., data=pcadata, CV=TRUE)
table(df$Type, carlda$class)
LogLoss(carlda$posterior[,2], pcadata$y)
data1 <- df
data1$Type  <- as.character(data1$Type)
data1$Type[data1$Type != "Small"] <- 0
data1$Type[data1$Type == "Small"] <- 1
data1$Type <- as.numeric(data1$Type)
library(MLmetrics)
x <- pcacar$x[,1:2]
y <- data1$Type
pcadata <- data.frame(x,y)
pred <- rep(0, nrow(pcadata))
for(i in 1:nrow(pcadata)) {
model <- glm(y ~.,family=binomial,data=pcadata[-i,])
pred[i] <- predict(model, newdata=pcadata[i,], type='response')
}
LogLoss(pred,pcadata$y)
library(MASS)
carlda<- lda(y~., data=pcadata, CV=TRUE)
table(df$Type, carlda$class)
LogLoss(carlda$posterior[,2], pcadata$y)
library(MASS)
carlda<- lda(y~., data=pcadata, CV=TRUE)
table(data1$Type, carlda$class)
LogLoss(carlda$posterior[,2], pcadata$y)
library(MASS)
carlda<- lda(y~., data=pcadata, CV=TRUE)
table(df$Type, carlda$class)
LogLoss(carlda$posterior[,2], pcadata$y)
library(MASS)
carlda<- lda(y~., data=pcadata, CV=TRUE)
table(data1$Type, carlda$class)
LogLoss(carlda$posterior[,2], pcadata$y)
library(neuralnet)
library(MLmetrics)
set.seed(4521)
nn <- neuralnet(Price~MPG.city+MPG.highway+EngineSize+Horsepower+RPM+Rev.per.mile+Fuel.tank.capacity+Length+Wheelbase+Width+Turn.circle+Rear.seat.room+Luggage.room+Weight, data = scar, hidden=5)
plot(nn)
MSE <- sum((compute(nn, scar[,2:15])$net.result-scar$Price)^2)/nrow(scar)
MSE
round(pcacar$rotation[,2], 2)
summary(pcacar)
summary(pcacar)
summary(pcacar)
data1 <- df
data1$Type  <- as.character(data1$Type)
data1$Type[data1$Type != "Small"] <- 0
data1$Type[data1$Type == "Small"] <- 1
data1$Type <- as.numeric(data1$Type)
library(MLmetrics)
x <- pcacar$x[,1:2]
y <- data1$Type
pcadata <- data.frame(x,y)
pred <- rep(0, nrow(pcadata))
for(i in 1:nrow(pcadata)) {
model <- glm(y ~.,family=binomial,data=pcadata[-i,])
pred[i] <- predict(model, newdata=pcadata[i,], type='response')
}
LogLoss(pred,pcadata$y)
library(MASS)
carlda<- lda(y~., data=pcadata, CV=TRUE)
table(data1$Type, carlda$class)
LogLoss(carlda$posterior[,2], pcadata$y)
library(MASS)
allcat <- df
allcat$Type <- as.character(allcat$Type)
allcat$Type[allcat$Type == "Compact"] <- 0
allcat$Type[allcat$Type == "Midsize"] <- 1
allcat$Type[allcat$Type == "Large"] <- 2
allcat$Type[allcat$Type == "Small"] <- 3
allcat$Type[allcat$Type == "Sporty"] <- 4
allcat$Type <- as.numeric(allcat$Type)
x <- pcacar$x[,1:2]
y <- allcat$Type
alldat <- data.frame(x,y)
alllda <- lda(y~., data=alldat, CV=TRUE)
MultiLogLoss(y_true = y, y_pred = alllda$posterior)
table(allcat$Type, alllda$class)
set.seed(217)
ind <- sample(1:nrow(scar), 41)
train <- scar[ind,]
test <- scar[-ind,]
#Training
set.seed(4521)
nn_train <- neuralnet(Price~MPG.city+MPG.highway+EngineSize+Horsepower+RPM+Rev.per.mile+Fuel.tank.capacity+Length+Wheelbase+Width+Turn.circle+Rear.seat.room+Luggage.room+Weight, data = train, hidden=5)
plot(nn_train)
MSE_train <- MSE(nn_train$response,compute(nn_train, train[,2:15])$net.result)
MSE_train #0.001227645277
#Testing
set.seed(4521)
nn_test <- neuralnet(Price~MPG.city+MPG.highway+EngineSize+Horsepower+RPM+Rev.per.mile+Fuel.tank.capacity+Length+Wheelbase+Width+Turn.circle+Rear.seat.room+Luggage.room+Weight, data = test, hidden=5)
MSE_test <- MSE(nn_test$response,compute(nn_test, test[,2:15])$net.result)
MSE_test #0.001167493148
set.seed(217)
ind <- sample(1:nrow(scar), 41)
train <- scar[ind,]
test <- scar[-ind,]
#Training
set.seed(4521)
nn_train <- neuralnet(Price~MPG.city+MPG.highway+EngineSize+Horsepower+RPM+Rev.per.mile+Fuel.tank.capacity+Length+Wheelbase+Width+Turn.circle+Rear.seat.room+Luggage.room+Weight, data = train, hidden=5)
#plot(nn_train)
MSE_train <- MSE(nn_train$response,compute(nn_train, train[,2:15])$net.result)
MSE_train #0.001227645277
#Testing
set.seed(4521)
nn_test <- neuralnet(Price~MPG.city+MPG.highway+EngineSize+Horsepower+RPM+Rev.per.mile+Fuel.tank.capacity+Length+Wheelbase+Width+Turn.circle+Rear.seat.room+Luggage.room+Weight, data = test, hidden=5)
MSE_test <- MSE(nn_test$response,compute(nn_test, test[,2:15])$net.result)
MSE_test #0.001167493148
library(neuralnet)
library(MLmetrics)
set.seed(4521)
nn <- neuralnet(Price~MPG.city+MPG.highway+EngineSize+Horsepower+RPM+Rev.per.mile+Fuel.tank.capacity+Length+Wheelbase+Width+Turn.circle+Rear.seat.room+Luggage.room+Weight, data = scar, hidden=5)
plot(nn)
MSE <- sum((compute(nn, scar[,2:15])$net.result-scar$Price)^2)/nrow(scar)
MSE
difference <- nn_test$response - compute(nn_test, test[,2:15])$net.result
sum(difference)/nrow(test) # -0.0001760476991
#Lets unscale this value
abs(sum(diff)/nrow(test))*(max(numeric$Price)-min(numeric$Price))+min(numeric$Price)
difference <- nn_test$response - compute(nn_test, test[,2:15])$net.result
sum(difference)/nrow(test) # -0.0001760476991
#Lets unscale this value
#we use absolute value since we have to scale first
abs(sum(diff)/nrow(test))*(max(numeric$Price)-min(numeric$Price))+min(numeric$Price)
df <- read.csv("car93.csv")
numeric <- df[sapply(df,is.numeric)]
pcacar <- prcomp(numeric, scale. = TRUE) #scaled
summary(pcacar)
#observing the principal components
#round(pcacar$rotation,2)
biplot(pcacar)
difference <- nn_test$response - compute(nn_test, test[,2:15])$net.result
sum(difference)/nrow(test) # -0.0001760476991
#Lets unscale this value
#we use absolute value since we have to scale first
abs(sum(difference)/nrow(test))*(max(numeric$Price)-min(numeric$Price))+min(numeric$Price)
difference <- nn_test$response - compute(nn_test, test[,2:15])$net.result
sum(difference)/nrow(test) # -0.0001760476991
#Lets unscale this value
#we use absolute value since we have to scale first
abs(sum(difference)/nrow(test))*(max(numeric$Price)-min(numeric$Price))+min(numeric$Price)
knitr::opts_chunk$set(echo = TRUE)
table1
anovatable <- data.frame("Source" =c("Treatments", "Residual", "Total"), "DF" = c(2,6,8), "SS" = c(10,40, 50), "MS" = c(5,40/6), "F" = c(5/(40/6),))
anovatable <- data.frame("Source" =c("Treatments", "Residual", "Total"), "DF" = c(2,6,8), "SS" = c(10,40, 50), "MS" = c(5,40/6), "F" = c(5/(40/6),10))
anovatable <- data.frame("Source" =c("Treatments", "Residual", "Total"), "DF" = c(2,6,8), "SS" = c(10,40, 50), "MS" = c(5,40/6), "F" = c(5/(40/6),10,10))
anovatable <- data.frame("Source" =c("Treatments", "Residual", "Total"), "DF" = c(2,6,8), "SS" = c(10,40, 50), "MS" = c(5,40/6), "F" = c(5/(40/6),10,10))
anovatable <- data.frame("Source" =c("Treatments", "Residual", "Total"), "DF" = c(2,6,8), "SS" = c(10,40, 50), "MS" = c(5,40/6), "F" = c(5/(40/6),10,10))
anovatable <- data.frame("Source" =c("Treatments", "Residual", "Total"), "DF" = c(2,6,8), "SS" = c(10,40, 50), "MS" = c(5,40/6,10), "F" = c(5/(40/6),10,10))
anovatable <- data.frame("Source" =c("Treatments", "Residual", "Total"), "DF" = c(2,6,8), "SS" = c(10,40, 50), "MS" = c(5,40/6,10), "F" = c(5/(40/6),10,10))
anovatable
anovatable <- data.frame("Source" =c("Treatments", "Residual", "Total"), "DF" = c(2,6,8), "SS" = c(10,40, 50), "MS" = c(5,40/6,40/6 + 5), "F" = c(5/(40/6),NA,NA))
anovatable
1 - pf(.75, 2, 6)
anovatable <- data.frame("Source" =c("Treatments", "Residual", "Total"), "DF" = c(2,6,8), "SS" = c(10,40, 50), "MS" = c(5,40/6,40/6 + 5), "F" = c(5/(40/6),NA,NA), "p-value" = c(1 - pf(.75, 2, 6)), NA,NA))
anovatable <- data.frame("Source" =c("Treatments", "Residual", "Total"), "DF" = c(2,6,8), "SS" = c(10,40, 50), "MS" = c(5,40/6,40/6 + 5), "F" = c(5/(40/6),NA,NA), "p-value" = c(1 - pf(.75, 2, 6), NA,NA))
anovatable
anovatable <- data.frame("Source" =c("Treatments", "Residual", "Total"), "DF" = c(2,6,8), "SS" = c(10,40, 50), "MS" = c(5,40/6,NA), "F" = c(5/(40/6),NA,NA), "p-value" = c(1 - pf(.75, 2, 6), NA,NA))
anovatable
#Removing the blocks
anovatable <- data.frame("Source" =c("Treatments", "Residual", "Total"), "DF" = c(2,6,8), "SS" = c(10,40, 50), "MS" = c(5,40/6,NA), "F" = c(5/(40/6),NA,NA), "p-value" = c(1 - pf(.75, 2, 6), NA,NA))
anovatable
sigma.hat <- 9*(var(GB) + var(CD))/18
sigma.hat
sigma.hat <- (9*var(GB) + 9*var(CD))/18
sigma.hat
sigma.hat <- (9*var(GB) + 9*var(CD))/18
sigma.hat
sigma.hat <- 9*(var(GB) + var(CD))/18
sigma.hat
#pooled variance
pooled.sigma <- 9*(var(GB) + var(CD))/18
pooled.sigma
#pooled variance
pooled.sigma <- 9*(var(GB) + var(CD))/18
pooled.sigma
ci.l <- tau1.hat-tau2.hat-qt(0.975, 18)*sqrt(sigma.hat)*sqrt(1/5)
ci.u <- tau1.hat-tau2.hat+qt(0.975, 18)*sqrt(sigma.hat)*sqrt(1/5)
round(c(ci.l,ci.u),3)
t <-qt(0.975, 18)
ci.l <- tau1.hat-tau2.hat-t*sqrt(sigma.hat)*sqrt(1/5)
ci.u <- tau1.hat-tau2.hat+t*sqrt(sigma.hat)*sqrt(1/5)
round(c(ci.l,ci.u),3)
t <-qt(0.975, 18)
ci.l <- tau1.hat-tau2.hat-t*sqrt(sigma.hat)*sqrt(1/5)
ci.u <- tau1.hat-tau2.hat+t*sqrt(sigma.hat)*sqrt(1/5)
round(c(ci.l,ci.u),3)
t <-qt(0.975, 18)
ci.l <- tau1.hat-tau2.hat-t*sqrt(sigma.hat)*sqrt(1/5)
ci.u <- tau1.hat-tau2.hat+t*sqrt(sigma.hat)*sqrt(1/5)
round(c(ci.l,ci.u),3)
data.frame()
profits<- c(100, 95, 125, 105, 100, 90, 135, 120, 85, 178,90, 110, 85, 90, 95, 110, 115, 110, 105, 178,250, 175, 140, 200, 195, 165, 145, 180, 210, 180)
table1 <- data.frame("Promotion" = c("Goose Bar","Goose Bar","Goose Bar","Goose Bar","Goose Bar","Goose Bar","Goose Bar","Goose Bar","Goose Bar","Goose Bar","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","Carbwich","Carbwich","Carbwich","Carbwich","Carbwich","Carbwich","Carbwich","Carbwich","Carbwich","Carbwich"), "Profits" = profits)
table1
summary(aov(table1$Profits ~ table1$Promotion))
data.frame()
profits<- c(100, 95, 125, 105, 100, 90, 135, 120, 85, 178,90, 110, 85, 90, 95, 110, 115, 110, 105, 178,250, 175, 140, 200, 195, 165, 145, 180, 210, 180)
table1 <- data.frame("Promotion" = c("Goose Bar","Goose Bar","Goose Bar","Goose Bar","Goose Bar","Goose Bar","Goose Bar","Goose Bar","Goose Bar","Goose Bar","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","$9 Dinner","Carbwich","Carbwich","Carbwich","Carbwich","Carbwich","Carbwich","Carbwich","Carbwich","Carbwich","Carbwich"), "Profits" = profits)
table1
summary(aov(table1$Profits ~ table1$Promotion))
mu.hat = mean(c(dataQ1[1,], dataQ1[2,]))
mu.hat
tau.goosebar = mean(dataQ1[1,]) - mu.hat
tau.goosebar
tau.9dinner = mean(dataQ1[2,]) - mu.hat
tau.9dinner
s2.hat.goosebar = var(dataQ1[1,])
s2.hat.9dinner = var(dataQ1[2,])
n = length(dataQ1[1,]) + length(dataQ1[2,])
s2.hat = ((length(dataQ1[2,])-1)*s2.hat.9dinner+(length(dataQ1[1,])-1)*s2.hat.goosebar)/( n - 2 )
s2.hat
alpha = 0.05
diff.tau.hat = tau.goosebar - tau.9dinner
se.diff.tau = sqrt(s2.hat*(1/length(dataQ1[2,]) + 1/length(dataQ1[1,])))
cval = qt(1-alpha/2, df=length(dataQ1[2,]) + length(dataQ1[1,]) - 2)
cval
round( diff.tau.hat + c(-1,+1)*cval*se.diff.tau, 2)
t.test(dataQ1[1,],dataQ1[2,], var.equal=TRUE)
dataQ2 = matrix(NA, nrow=1, ncol=10)
dataQ2[1,] = c(250, 175, 140, 200, 195, 165, 145, 180, 210, 180)
rownames(dataQ2) = c("Carbwich")
dataQ2
dataQ12 <-t(rbind(dataQ1,dataQ2))
boxplot(dataQ12,xlab="Promotion",ylab="Profit (Feridollars)")
boxplot( dataQ1[1,],dataQ1[2,], dataQ2[1,], names = c("Goose Bar", "$9 Dinner","Carbwich"),main = "Promotion Boxplots",xlab="Promotion",ylab="Profit (Feridollars)")
boxplot( dataQ1[1,],dataQ1[2,], dataQ2[1,], names = c("Goose Bar", "$9 Dinner","Carbwich"),main = "Promotion Boxplots",xlab="Promotion",ylab="Profit (Feridollars)")
dataQ12 <-t(rbind(dataQ1,dataQ2))
boxplot(dataQ12,xlab="Promotion",ylab="Profit (Feridollars)")
t <- c(rep("goose bar", 10), rep("cheap dinner", 10), rep("carbwich", 10))
t
p <- as.numeric(dataQ12)
data.frame()
profits<- c(100, 95, 125, 105, 100, 90, 135, 120, 85, 178,90, 110, 85, 90, 95, 110, 115, 110, 105, 178,250, 175, 140, 200, 195, 165, 145, 180, 210, 180)
table1 <- data.frame("Promotion" = c(rep("goose bar", 10),rep("cheap dinner", 10), rep("carbwich", 10)), "Profits" = profits)
table1
summary(aov(table1$Profits ~ table1$Promotion))
p <- as.numeric(dataQ12)
t <- c(rep("goose bar", 10), rep("cheap dinner", 10), rep("carbwich", 10))
summary(aov(p ~ t))
data.frame()
profits<- c(100, 95, 125, 105, 100, 90, 135, 120, 85, 178,90, 110, 85, 90, 95, 110, 115, 110, 105, 178,250, 175, 140, 200, 195, 165, 145, 180, 210, 180)
table1 <- data.frame("Promotion" = c(rep("goose bar", 10),rep("cheap dinner", 10), rep("carbwich", 10)), "Profits" = profits)
summary(aov(table1$Profits ~ table1$Promotion))
dataQ12 <-t(rbind(dataQ1,dataQ2))
dataQ12
dataQ12 <-(rbind(dataQ1,dataQ2))
dataQ12
data.frame()
profits<- c(100, 95, 125, 105, 100, 90, 135, 120, 85,101,90, 110, 85, 90, 95, 110, 115, 110, 105, 120,250, 175, 140, 200, 195, 165, 145, 180, 210, 180)
table1 <- data.frame("Promotion" = c(rep("goose bar", 10),rep("cheap dinner", 10), rep("carbwich", 10)), "Profits" = profits)
summary(aov(table1$Profits ~ table1$Promotion))
df <- read.csv("train_cleaned.csv")
setwd("desktop/kaggle-house-price")
setwd("desktop/kaggle-house-prices")
df <- read.csv("train_cleaned.csv")
numeric = df[c('LotFrontage', 'LotArea','MasVnrArea',
'BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF',
'1stFlrSF', 'LowQualFinSF','GrLivArea','TotRmsAbvGrd',
'GarageYrBlt','GarageArea','WoodDeckSF','OpenPorchSF',
'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea',
'MiscVal','TotalSF','Age','RemodAge')]
df
df <- pd.read.csv("numeric_forpca.csv")
df <- read.csv("numeric_forpca.csv")
train <- read.csv("train_cleaned.csv")
x <- df
housepca <- prcomp(x, scale.=TRUE)
summary(housepca)
train <- read.csv("train_cleaned.csv")
df <- read.csv("numeric_forpca.csv")
#So we have 23 princpial components
housepca <- prcomp(df, scale.=TRUE)
summary(housepca)
# And as mentioned in the ipython notebook
# we retain 90% of the variability with 15 principal components
#store the scores
scr <- df$x[,1:15]
train
pcregmod <- lm(train$SalePrice ~ scr)
scr <- df$x[,1:15]
pcregmod <- lm(train$SalePrice ~ scr)
scr
#Since we have already saved the numeric
#dataset from running pca in python lets
#just run this to make sure everyhting's working correctly
train <- read.csv("train_cleaned.csv")
df <- read.csv("numeric_forpca.csv")
#So we have 23 princpial components
housepca <- prcomp(df, scale.=TRUE)
summary(housepca)
# And as mentioned in the ipython notebook
# we retain 90% of the variability with 15 principal components
#store the scores
scr <- df$x[,1:15]
scr
df$x[,1:15]
scr <- housepca$x[,1:15]
scr
summary(housepca)
plot(nupca, type="lines")
plot(housepca, type="lines")
#Since we have already saved the numeric
#dataset from running pca in python lets
#just run this to make sure everyhting's working correctly
train <- read.csv("train_cleaned.csv")
df <- read.csv("numeric_forpca.csv")
#So we have 23 princpial components
housepca <- prcomp(df, scale.=TRUE)
summary(housepca)
# And as mentioned in the ipython notebook
# we retain 90% of the variability with 15 principal components
# but apparently the scree plot tells us that we should probably pick 4 principal components since that's where the elbow is
plot(housepca, type="lines")
# But if we were to use the Kaiser criterion we would only keep up to 8 components
# So different from the 90% variablity we discussed in the notebbook we will
# take the averaage amount of principal componets from the different decision methods
# so lets take 8 predicotors
#store the scores
scr <- housepca$x[,1:8]
scr
# Principal Componenet Regression
#now lets run the multiple linear regression on the pc's
#lets first get the response varialbe from the training set
pcregmod <- lm(train$SalePrice ~ scr)
summary(pcregmod)
#Since we have already saved the numeric
#dataset from running pca in python lets
#just run this to make sure everyhting's working correctly
train <- read.csv("train_cleaned.csv")
df <- read.csv("numeric_forpca.csv")
#So we have 23 princpial components
housepca <- prcomp(df, scale.=TRUE)
summary(housepca)
# And as mentioned in the ipython notebook
# we retain 90% of the variability with 15 principal components
# but apparently the scree plot tells us that we should probably pick 4 principal components since that's where the elbow is
plot(housepca, type="lines")
# But if we were to use the Kaiser criterion we would only keep up to 8 components
# So different from the 90% variablity we discussed in the notebbook we will
# take the averaage amount of principal componets from the different decision methods
# so lets take 8 predicotors
#store the scores
scr <- housepca$x[,1:15]
scr
# Principal Componenet Regression
#now lets run the multiple linear regression on the pc's
#lets first get the response varialbe from the training set
pcregmod <- lm(train$SalePrice ~ scr)
summary(pcregmod)
# Observing the output only PC1 is significant based on the pvalue of the null hypothesis
# And R squared is extre
#Since we have already saved the numeric
#dataset from running pca in python lets
#just run this to make sure everyhting's working correctly
train <- read.csv("train_cleaned.csv")
df <- read.csv("numeric_forpca.csv")
#So we have 23 princpial components
housepca <- prcomp(df, scale.=TRUE)
summary(housepca)
# And as mentioned in the ipython notebook
# we retain 90% of the variability with 15 principal components
# but apparently the scree plot tells us that we should probably pick 4 principal components since that's where the elbow is
plot(housepca, type="lines")
# But if we were to use the Kaiser criterion we would only keep up to 8 components
# So different from the 90% variablity we discussed in the notebbook we will
# take the averaage amount of principal componets from the different decision methods
# so lets take 8 predicotors
#store the scores
scr <- housepca$x[,1:8]
scr
# Principal Componenet Regression
#now lets run the multiple linear regression on the pc's
#lets first get the response varialbe from the training set
pcregmod <- lm(train$SalePrice ~ scr)
summary(pcregmod)
# Observing the output only PC1 is significant based on the pvalue of the null hypothesis
# And R squared is extremely low
#
summary(housepca)
summary(pcregmod)
#just run this to make sure everyhting's working correctly
train <- read.csv("train_cleaned.csv")
df <- read.csv("numeric_forpca.csv")
#So we have 23 princpial components
housepca <- prcomp(df, scale.=TRUE)
summary(housepca)
nupca$rotation[,1:3]
housepca$rotation[,1:3]
scr <- housepca$x[,1:8]
df
train <- read.csv("train_cleaned.csv")
df <- read.csv("numeric_forpca.csv")
train <- read.csv("train_cleaned.csv")
df <- read.csv("numeric_forpca.csv")
setwd("/users/chi/desktop/kaggle-house-prices/principal component analysis and regression")
#just run this to make sure everyhting's working correctly
train <- read.csv("train_cleaned.csv")
df <- read.csv("numeric_forpca.csv")
housepca <- prcomp(df, scale.=TRUE)
summary(housepca)
plot(housepca, type="lines")
housepca$rotation[,1:3]
housepca$rotation[,1:4]
# Now maybe we take only what the Kaiser Criterion tells us to
scr_Kaiser <- housepca$x[,1:4]
pcregKaiser <- lm(train$SalePrice ~ scr_Kaiser)
summary(pcregKaiser)
scr_PC1<- housepca$x[,1:1]
pcregPC1 <- lm(train$SalePrice ~ scr_PC1)
summary(pcregPC1)
